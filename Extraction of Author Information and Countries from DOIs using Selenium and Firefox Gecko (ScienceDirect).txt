import requests
from selenium import webdriver
from selenium.webdriver.firefox.service import Service
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
import time
import csv

def get_first_author_from_crossref(doi):
    url = f"https://api.crossref.org/works/{doi}"
    try:
        response = requests.get(url)
        response.raise_for_status()
        data = response.json()
        first_author = data['message']['author'][0]['given'] + " " + data['message']['author'][0]['family']
        return first_author
    except Exception as e:
        print(f"Error fetching data for DOI: {doi} - {e}")
        return "Error occurred"

def get_author_info_from_doi_html(doi, first_author, sleep_time):
    url = f"https://doi.org/{doi}"
    try:
        options = webdriver.FirefoxOptions()
        options.binary_location = "C:/Program Files/Mozilla Firefox/firefox.exe"
        options.headless = False

        driver = webdriver.Firefox(service=Service('C:/Users/Leonardo/Desktop/ChromeDriver/geckodriver.exe'), options=options)
        driver.get(url)

        time.sleep(sleep_time)
        
        WebDriverWait(driver, 20).until(EC.presence_of_element_located((By.TAG_NAME, 'body')))
        
        scripts = driver.find_elements(By.TAG_NAME, 'script')
        for script in scripts:
            if 'application/json' in script.get_attribute('type'):
                script_content = script.get_attribute('innerHTML')
                if 'affiliation' in script_content:
                    start_index = script_content.find('affiliation') + len('affiliation') + 3
                    end_index = script_content.find(']', start_index) + 1
                    affiliation_info = script_content[start_index:end_index]
                    driver.quit()
                    return affiliation_info
        
        # Se não encontrar na tag script, tentar encontrar na classe 'affiliation'
        affiliation_divs = driver.find_elements(By.CLASS_NAME, 'affiliation')
        if affiliation_divs:
            affiliations = [div.text for div in affiliation_divs]
            driver.quit()
            return affiliations
        
        driver.quit()
        return "Not found"
    except Exception as e:
        print(f"Error fetching data for DOI: {doi} - {e}")
        return "Error occurred"

def extract_country(text):
    try:
        country = text.split(",")[-1].strip()
        return country
    except Exception as e:
        print(f"Error extracting country: {e}")
        return "Country not found"

# Função para remover 'https://' do DOI e processar corretamente a informação
def clean_doi(doi):
    return doi.replace('https://doi.org/', '')

# Lista de DOIs
dois = [
"https://doi.org/10.1016/j.jaap.2015.05.006",
LISTA DE DOI
"https://doi.org/10.1016/j.ecolind.2015.01.043"

]

# Limpar os DOIs
cleaned_dois = [clean_doi(doi) for doi in dois]

# Dicionário para armazenar os resultados
results = {}

sleep_time = 6

for doi in cleaned_dois:
    first_author = get_first_author_from_crossref(doi)
    if "Error" not in first_author:
        try:
            author_info = get_author_info_from_doi_html(doi, first_author, sleep_time)
            if "Error" not in author_info and "Not found" not in author_info:
                country = extract_country(author_info)
                results[doi] = country
            else:
                results[doi] = "NOT FOUND"
        except Exception as e:
            print(f"Error fetching data for DOI: {doi} - {e}")
            results[doi] = "NOT FOUND"
    else:
        results[doi] = "NOT FOUND"
    print(f"{doi}: {results[doi]}")

# Salvar os resultados em um arquivo CSV
with open('DOInCountry.csv', mode='w', newline='', encoding='utf-8') as file:
    writer = csv.writer(file)
    writer.writerow(["DOI", "Country"])
    for doi, country in results.items():
        writer.writerow([doi, country])
